# Laboratory Assignment 1 


## Problem Statement 
Choose any publicly available dataset from data sources and types that you have chosen.  
Create a python file named “data_exploration.py” to retrieve the dataset using its API and 
store it in csv / excel format.  
Run some basic operations on the dataset including displaying the first few records, 
calculating the size and dimensions of the dataset, identifying missing data, etc.  
For websites, extract the text using web scraping libraries.  
For PDF documents, extract text using the any PDF to text libraries.   

## Video Demo Link
  TBD

## Technologies Used
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/)
[![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)](https://www.python.org/)
![Selenium](https://img.shields.io/badge/Selenium-43B02A?style=for-the-badge&logo=selenium&logoColor=white)
![Ubuntu](https://img.shields.io/badge/Ubuntu-E95420?style=for-the-badge&logo=ubuntu&logoColor=white)
![VMware Workstation Pro](https://img.shields.io/badge/VMware_Workstation_Pro-607078?style=for-the-badge&logo=vmware&logoColor=white)
![Beautiful Soup](https://img.shields.io/badge/Beautiful_Soup-004B87?style=for-the-badge&logo=python&logoColor=white)
```
📦 
Lab 1
   ├─ .gitignore
   ├─ README.md
   ├─ Screenshots
   ├─ lab1-s25.pdf
   └─ shubhamdarekar_1641138809
      ├─ data
      │  ├─ processed_data
      │  │  ├─ market_data.csv
      │  │  └─ news_data.csv
      │  └─ raw_data
      │     └─ web_data.html
      ├─ scripts
      │  ├─ data_filter.py
      │  ├─ task_1.py
      │  └─ web_scraper.py
      └─ venv
```
©generated by [Project Tree Generator](https://woochanleee.github.io/project-tree-generator)



## How to run Application Locally
1. Clone the GitHub Project
2. Install python and the requirements
3. Run the files in script folder

## Code details
1. Scrape Data => Using a headless Chromium browser using Selenium, it navigates to the news website waits for 5 seconds to let the website load and parses the HTML content using Beautiful Soup. The code writes the data in raw data folder
2. Filter Data => It reads the data from previously written html file. We parse the data from BeautifulSoup and uses three function, extract_market_data to get market data into dictionary, extract_latest_news to get the news data in dictionary and convert_to_csv to convert and write to csv files.

## References

- https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- Readme File template -> https://www.readme-templates.com/
